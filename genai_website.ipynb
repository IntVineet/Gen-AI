{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1079f340",
   "metadata": {},
   "source": [
    "# GenAI Website Safety Analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b557e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (0.28.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (4.13.4)\n",
      "Requirement already satisfied: requests in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (2.32.4)\n",
      "Requirement already satisfied: validators in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (0.35.0)\n",
      "Requirement already satisfied: tqdm in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from openai) (3.12.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vineetkumar/Library/Python/3.9/lib/python/site-packages (from aiohttp->openai) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai beautifulsoup4 requests validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0f5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API nhi bataunga\"\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"  \n",
    "model_name = \"llama3-70b-8192\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978571c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_with_ai(text_content):\n",
    "    prompt = f\"\"\"\n",
    "You are a cybersecurity assistant. Analyze the following website content and tell if it is potentially harmful, scammy, misleading, or safe. Give your result as:\n",
    "- Safety Status: Safe / Caution / Dangerous\n",
    "- Reason:\n",
    "- Suggestions:\n",
    "\n",
    "Content:\n",
    "{text_content[:3000]}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=300,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Groq API Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd6d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import validators\n",
    "\n",
    "def analyze_website(url):\n",
    "    if not validators.url(url):\n",
    "        return {\"error\": \"Invalid URL\"}\n",
    "\n",
    "    domain = url.split('//')[-1].split('/')[0]\n",
    "    is_https = url.startswith(\"https://\")\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
    "        desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = desc[\"content\"] if desc else \"No Description\"\n",
    "\n",
    "        icon_tag = soup.find(\"link\", rel=lambda x: x and 'icon' in x.lower())\n",
    "        favicon = icon_tag.get(\"href\", \"\") if icon_tag else \"Not Found\"\n",
    "        if favicon.startswith(\"/\"):\n",
    "            favicon = url.rstrip(\"/\") + favicon\n",
    "\n",
    "        text_content = soup.get_text(separator=' ', strip=True)\n",
    "        ai_review = analyze_with_ai(text_content)\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"domain\": domain,\n",
    "            \"is_https\": is_https,\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"favicon\": favicon,\n",
    "            \"ai_review\": ai_review,\n",
    "            \"status\": \"Groq-reviewed\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4afc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ai_review': '**Safety Status: Safe**\\n'\n",
      "              '\\n'\n",
      "              '**Reason:** The provided content appears to be a legitimate '\n",
      "              'Wikipedia page, listing various languages and their '\n",
      "              'corresponding article counts. The content is informative, '\n",
      "              'neutral, and does not contain any suspicious links, phishing '\n",
      "              'attempts, or malicious code.\\n'\n",
      "              '\\n'\n",
      "              '**Suggestions:**\\n'\n",
      "              '\\n'\n",
      "              '* Verify the authenticity of the website by checking the URL '\n",
      "              'and ensuring it starts with \"https\" and \"wikipedia.org\".\\n'\n",
      "              '* Be cautious when clicking on links from unfamiliar sources, '\n",
      "              'even if they appear to be from Wikipedia.\\n'\n",
      "              '* Keep your browser and operating system up to date to ensure '\n",
      "              'you have the latest security patches and protections.\\n'\n",
      "              \"* If you're unsure about the legitimacy of a website or \"\n",
      "              'content, you can always check with a trusted cybersecurity '\n",
      "              'resource or report it to the relevant authorities.',\n",
      " 'description': 'Wikipedia is a free online encyclopedia, created and edited '\n",
      "                'by volunteers around the world and hosted by the Wikimedia '\n",
      "                'Foundation.',\n",
      " 'domain': 'www.wikipedia.org',\n",
      " 'favicon': 'https://www.wikipedia.org/static/apple-touch/wikipedia.png',\n",
      " 'is_https': True,\n",
      " 'status': 'Groq-reviewed',\n",
      " 'title': 'Wikipedia',\n",
      " 'url': 'https://www.wikipedia.org'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = analyze_website(\"https://www.wikipedia.org\")\n",
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
